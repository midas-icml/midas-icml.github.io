<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="Mastering Robot Manipulation with Multimodal Prompts through Pretraining and Multi-task Fine-tuning.">
  <meta name="keywords" content="Embodied AI, Multimodal Learning, Robotics Control">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Mastering Robot Manipulation with Multimodal Prompts through Pretraining and Multi-task Fine-tuning
  </title>
  <!-- custom fonts -->
  <link rel="stylesheet" type="text/css"
    href="https://cdn.rawgit.com/dreampulse/computer-modern-web-font/master/fonts.css">
  <link href="https://fonts.cdnfonts.com/css/proxima-nova-2" rel="stylesheet">
  <!-- end custom fonts -->

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" type="svg" href="./static/fire.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Mastering Robot Manipulation with Multimodal Prompts through
              Pretraining and Multi-task Fine-tuning
            </h1>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://sites.google.com/view/jiachenli/home">Jiachen Li</a><sup>1</sup>
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=Ub3LlsgAAAAJ&hl=en/">Qiaozi Gao</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=hEous9QAAAAJ&hl=en/">Michael
                  Johnston</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://xfgao.github.io/">Xiaofeng Gao</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=kDzxOzUAAAAJ&hl=en/">Xuehai He</a><sup>3</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=7r5shcMAAAAJ&hl=en/">Hangjie Shi</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=IP6H8LYAAAAJ&hl=en/">Suhaila Shakiah</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=00ncu3cAAAAJ&hl=en/">Reza Ghanadan</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://sites.cs.ucsb.edu/~william/">William Yang Wang</a><sup>1</sup>
              </span>
            </div>


            <div class="is-size-5 publication-authors" style="margin-top: 15px;">
              <span class="author-block"><sup>1</sup>UC Santa Barbara,</span>
              <span class="author-block"><sup>2</sup>Amazon AGI,</span>
              <span class="author-block"><sup>3</sup>UC Santa Cruz</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a target="_blank" href="https://arxiv.org/pdf/2310.09676"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a target="_blank" href="https://arxiv.org/abs/2310.09676"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a target="_blank" href="https://github.com/Ji4chenLi/"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Poster Link. -->
                <span class="link-block">
                  <a target="_blank" href="static/images/poster_icml2024_midas.pdf"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Poster</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Prompt-based learning has been demonstrated as a compelling paradigm contributing to large language
              models' tremendous success (LLMs).
              Inspired by their success in language tasks, existing research has leveraged LLMs in embodied instruction
              following and task planning.
              In this work, we tackle the problem of training a robot to understand multimodal prompts, interleaving
              vision signals with text descriptions.
              This type of task poses a major challenge to robots' capability to understand the interconnection and
              complementarity between vision and language signals.
              In this work, we introduce an effective framework that learns a policy to perform robot manipulation with
              multimodal prompts from multi-task expert trajectories.
              Our methods consist of a two-stage training pipeline that performs inverse dynamics pretraining and
              multi-task finetuning. To facilitate multimodal understanding,
              we design our multimodal prompt encoder by augmenting a pretrained LM with a residual connection to the
              visual input and model the dependencies among action dimensions.
              Empirically, we evaluate the efficacy of our method on the <a href="https://vimalabs.github.io/"
                target="_blank">VIMA-BENCH</a> and establish a new state-of-the-art
              (10% improvement in success rate). Moreover, we demonstrate that our model exhibits remarkable in-context
              learning ability.
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">

      <!-- Challenges. -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Challenges in Multimodal Robotic Learning</h2>

          <div class="content has-text-justified">
            <ul>
              <li>The vision signals in the prompt can represent target objects, delineate a specific sub-goal, or offer
                in-context demonstrations.</li>
              <li>The robot must understand the underlying transition dynamics suggested by the multimodal prompts
                before tackling the overall task objective</li>
              <li>The robot should infer state transitions from language instructions, and deducing actions from image
                demonstrations, a concept known as inverse dynamic prediction.</li>
              <li> However, <b>imitation learning falls short</b> in teaching robots to <b>predict inverse dynamics</b>,
                as <b>future observations are often masked out</b> when training to predict actions from current and
                history observations</li>
            </ul>

          </div>
        </div>
      </div>
      <!--/ Challenges. -->

      <!-- Pretraining. -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Inverse Dynamic Pretraining</h2>
          <img src="static/images/inverse_dynamic_prediction.png" alt="" srcset="">
          <div class="content has-text-justified">
            <ul>
              <li>We make a novel observation that <b>every robot trajectory itself can be reformulated into a motion
                  following task</b>.</li>
              <li>
                <p>
                  Given any sequence of robot trajectory
                  <span>&#969;<sub>T</sub> = (o<sub>0</sub>, a<sub>0</sub>, o<sub>1</sub>, &#x2026;, a<sub>T-1</sub>,
                    o<sub>T</sub>)</span>,
                  we can always create a task with the prompt
                  <span>q<sub>pretrain</sub> = (<i>Follow this motion: </i>o<sub>0</sub>, &#x2026;,
                    o<sub>T</sub>)</span>
                  and ground-truth actions
                  <span>(a<sub>0</sub>, &#x2026;, a<sub>T-1</sub>)</span>.
                </p>
              </li>
            </ul>
          </div>
        </div>
      </div>
      <!--/ Pretraining. -->

      <!-- Model Architecture. -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Model Architecture</h2>

          <img src="./static/images/model_arc.jpg" alt="" srcset="">
          <div class="content has-text-justified">
            <p>
              <b>Model Architecture of our MIDAS</b>. Our model adopts a decoder-only architecture. The multimodal
              prompt embeddings are concatenated with history observation and action tokens. We model each action
              dimension as an individual token and predict them auto-regressively.
            </p>
          </div>
        </div>
      </div>
      <!--/ Model Architecture. -->

      <!-- Multimodal Prompt Encoder. -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Multimodal Prompt Encoder</h2>

          <img src="./static/images/multimodal_prompt_encoder.png" alt="" srcset="">
          <div class="content has-text-justified">
            <p>
              (a) We adopt the <b>Object Encoder</b> proposed in VIMA, which consists of a ViT that extracts visual embedding from cropped object images and a MLP that encodesbounding boxes. The two embeddings are concatenated before passing through a Fusion MLP to get the object tokens. (b) Our Multimodal Prompt Encoder adds a residual connection from the input object tokens to the pretrained LM output.
            </p>
          </div>
        </div>
      </div>
      <!--/ Multimodal Prompt Encoder. -->

      <!-- Individual Action token. -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Modeling the Dependency Among Each Action Dimension</h2>

          <img src="./static/images/individual_action_token.png" alt="" srcset="">
          <div class="content has-text-justified">
            <p>
              <b>Independently predicting each action dimension can be problematic.</b> Consider the example above, the robot should move either the heart
              or the cross block. As the policy predicts each action dimension independently, different dimensions do not consistently
              manipulate the same object, resulting in a task failure.
            </p>
          </div>
        </div>
      </div>
      <!--/ Individual Action token. -->

      <!-- Experimental results. -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Experiment Results</h2>

          <img src="./static/images/main-results.png" alt="" srcset="">
          <div class="content has-text-justified">
            <p>
              We compared our methods with baseline approaches on the VIMA-BENCH across all four evaluation levels.
              “Avg”
              represents the average success rate for all tasks within an evaluation level. To determine the success
              rate for each method,
              we sampled 200 episodes from every task. Our methods significantly outperform baseline methods and
              <b>establish a
                new state-of-the-art performance on the VIMA-BENCH.</b>
            </p>
          </div>
        </div>
      </div>
      <!--/ Experimental results. -->

      <div class="container is-max-desktop">

        <!-- Acknowledgement. -->
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 class="title is-3">Acknowledgement</h2>

            <div class="content has-text-justified">
              <p>
                This project would not be possible without the wonderful prior work <a
                  href="https://vimalabs.github.io/" target="_blank">VIMA-BENCH</a>.
              </p>
            </div>
          </div>
        </div>
        <!--/ Acknowledgement. -->
  </section>

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{li2024midas,
  title={Mastering Robot Manipulation with Multimodal Prompts through Pretraining and Multi-task Fine-tuning},
  author={Jiachen Li and Weixi Feng and Wenhu Chen and William Yang Wang},
  journal={Proceedings of the 41st International Conference on Machine Learning},
  year={2024}
}
      </code></pre>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="https://proceedings.mlr.press/v202/li23av/li23av.pdf">
          <i class="fas fa-file-pdf"></i>
        </a>
        <a class="icon-link" href="https://github.com/cfpo-icml23/cfpi" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p style="text-align: center
              ">
              Website templated borrowed from <a href="https://github.com/nerfies/nerfies.github.io">nerfies.</a>
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>


</body>

</html>